{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineare Regression\n",
    "\n",
    "<!--- TODO TOC -->\n",
    "\n",
    "> Vorhersage einer numerischen Zielvariable in Abhängigkeit von einer oder mehreren Eingangsvariablen = Welchen Wert `y` erhält man für ein gegebenes `x` ?\n",
    "\n",
    "Trainingsdaten enthält:\n",
    "\n",
    "1. Feature: *Input*(Eingangsvariable)\n",
    "2. Label: *Output*(Zielvariable)\n",
    "\n",
    "---\n",
    "\n",
    "**TLDR:**\n",
    "\n",
    "* Ziel: Aus Datensatz(Paaren $(x,y)$) Vorhersagemodell welches möglichst genaue Vorhersagen macht trainieren:\n",
    "\n",
    "1. Datensatz in Trainings- & Testdaten aufzuteilen(80/20)\n",
    "2. Durch Lineare Regression auf Trainingsdaten **Modell trainieren**. Dabei Gradient Descent nutzen um optimalen Parameer $w_0,w_1$ zu finden, welcher die Kostenfunktion auf den Trainingsdaten minimiert\n",
    "3. **Vorhersagegenauigkeit** des trainierten Modells auf ungesehene Daten vornehmen und MSE ermitteln\n",
    "\n",
    "---\n",
    "\n",
    "## Fragestellung\n",
    "\n",
    "> Welche Gerade $f(x) = w_0 + w_1 x$ beschreibt die Daten(Bspw. Punkte auf 2d-Diagramm) am besten?\n",
    "\n",
    "## Lösungsmöglichkeit: MSE\n",
    "\n",
    "> Die Gerade, die den Abstand(*Fehler*) zu Trainingsdaten minimiert\n",
    "\n",
    "* Durschnittlichen Qudratischen Fehler über alle n Datenpunkte entspricht dem **mean squared error**:\n",
    "\n",
    "$MSE =  \\frac{1}{n} \\sum_{i=0}^n e^2_i = \\frac{1}{n} \\sum_{i=0}^n (y_i - f(x_i))^2 = \\frac{1}{n} \\sum_{i=0}^n (y_i - (w_0 + w_1x_i))^2$\n",
    "\n",
    "* MSE hängt nur v. Parametern $w_0$ und $w_1$ ab und wird als **Kostenfunktion** *J*(**Cost function**) angegeben: $J(w_0, w_1) = \\frac{1}{n} \\sum_{i=0}^n (y_i - (w_0 + w_1x_i))^2$\n",
    "\n",
    "$\\Rightarrow$ Ziel: **minimalen Koten** zu finden um den Fehler auf den Trainingsdaten zu minimieren.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "> **Optimum**(Paar v. $\\hat{w_0}$ und $\\hat{w_1}$) finden durch bspw. **Gradient Descent**:\n",
    "\n",
    "* An beliebigen Stelle starten und schrittweiße ein Richtung des steilsten Abstiegs sich bewegen bis keine Verbesserung mehr erreicht wird.\n",
    "* Negativer Gradient($\\approx$Steigung) ist Vektor, der in Richtung d. steilsten Abstiegs zeigt:\n",
    "\n",
    "<img  style=\"display: block; margin: auto;\" src=\"https://miro.medium.com/max/1400/1*HrFZV7pKPcc5dzLaWvngtQ.png\" width=\"400\" height=\"400\" />\n",
    "\n",
    "* Gradient einer Funktion, ist Vektor, dessen Einträge ersten **partiellen Ableitungen** der Funktion sind\n",
    "* Jeder Eintrag gibt dabei **Anstiegt** d. Funktion in Richtung d. Variablen an nach der abgeleitet wurde\n",
    "\n",
    "$\\Rightarrow$ $grad(f) = \\begin{pmatrix} f_x \\\\ f_y  \\end{pmatrix}$ berechnen, in dem man partielle Ableitung nach den jeweiligen Variablen macht!\n",
    "\n",
    "### Anwendung von Gradient Decent\n",
    "\n",
    "1. Wähle zufällige Startwerte für $\\hat{w_0}$ und $\\hat{w_1}$\n",
    "2. Bestimme Richtung d. steilsten Abstiegs(Negativer Gradient)\n",
    "3. Bewege($w_0, w_1$) in Richtung d. steilsten Abstiegs\n",
    "4. WDH Schritte 2 & 3 bis keine Verbeserung mehr erreicht wird\n",
    "\n",
    "Minimum der Kostenfuntion, durch Gradienten der Kostenfunktion bezüglich $w_0, w_1$:\n",
    "\n",
    "Siehe MSE: $J(w_0, w_1) = \\frac{1}{n} \\sum_{i=0}^n (y_i - (w_0 + w_1x_i))^2$\n",
    "\n",
    "> $\\frac{\\nabla}{\\nabla w_0} J(w_0, w_1) = \\frac{1}{n} \\sum_{i=1}^n -2(y_i - (w_0 + w_1 x_i))$\n",
    "\n",
    "> $\\frac{\\nabla}{\\nabla w_1} J(w_0, w_1) = \\frac{1}{n} \\sum_{i=1}^n -2x_i(y_i - (w_0 + w_1 x_i))$\n",
    "\n",
    "### Pseudocode zum Gradient Descent\n",
    "\n",
    "* $\\alpha$: Schrittgröße(*Lernrate*) in Richtung d. Gradienten\n",
    "* Konvergiert, wenn sich $w_0, w_1$ zwischen 2 Iterationen nicht mehr als ein gegebenes $\\epsilon$ ändert(Default sklearn: $0.001$).\n",
    "* Jeder Durchlauf der Schleife beschreibt neue Gereade, die näher an Trainingsdaten liegt als vorherige:\n",
    "\n",
    "![](./img/gd.png)\n",
    "\n",
    "### Lernrate\n",
    "\n",
    "> Lernrate $\\alpha$ bestimmt die Schrittgröße\n",
    "\n",
    "1. Lernrate zu klein $\\rightarrow$ Sehr langsame Annäherung an das Minimum\n",
    "2. Lernrate zu groß $\\rightarrow$ Gefahr, dass man über das Minimum hinaus springt\n",
    "\n",
    "## Modelltraining\n",
    "\n",
    "> Interne Parameter des ML-Modells so anpassen, dass diese bestmöglich Trainingsdaten beschreiben\n",
    "\n",
    "* Finden Parameter $\\hat{=}$ Modelltraing oder Lernen des ML-Modells\n",
    "\n",
    "### Training-Test-Split\n",
    "\n",
    "> Evaluieren wie gut Vorhersage des Modells auf neuen Datenpunkten ist\n",
    "\n",
    "! **Modell nicht auf selben Daten auf denen Model trainiert wird evaluieren** !\n",
    "\n",
    "* Vor Beginn des Trainings, alle zu Verfügung stehenden Daten zufällig in *Trainingsdaten* & *Testdaten* unterteilt(*80/20*)\n",
    "\n",
    "### Performance-Evaluation\n",
    "\n",
    "1. Mache für jeden Datenpunkt $x_i$ in Testdaten Vorhersage $f(x_i)$\n",
    "2. Bestimme *Vorhersagefehler* $y_i - f(x_i)$\n",
    "3. Bestimme mittleren quadratischen Fehler(MSE) über alle Datenpunkte\n",
    "\n",
    "$\\Rightarrow$ Metrik(MSE) wurde auf Trainingsdaten angewandt. Jetzt gleiches Prinzip auf Testdaten\n",
    "\n",
    "## Ausblick\n",
    "\n",
    "### Mehrdimensionale Regression\n",
    "\n",
    "> Gleiche Prinzip wie beschrieben auf mehrere Variablen anwenden\n",
    "\n",
    "* n-Dimensionale Vorhersagefunktione nutzen\n",
    "\n",
    "<!-- TODO ausführlicher machen ? -->\n",
    "\n",
    "### Polynomale Regression\n",
    "\n",
    "> Andere Muster, bei denen keine direkten lineare Zusammenhänge bestehen\n",
    "\n",
    "<!-- TODO ausführlicher machen ? -->\n",
    "\n",
    "## Quellen\n",
    "\n",
    "* Bild: https://miro.medium.com/max/1400/1*HrFZV7pKPcc5dzLaWvngtQ.png\n",
    "* Pseudocode: ML Kurs Folien\n",
    "* General informations: https://towardsdatascience.com/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
